{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/wisaaco/AA_DistributedSystems_Lab/blob/main/U9-Spark/SparkExamples.ipynb)\n",
    "\n",
    "Si no funciona el botó podeu copiar el següent [enllaç](https://colab.research.google.com/github/wisaaco/AA_DistributedSystems_Lab/blob/main/U9-Spark/SparkExamples.ipynb)\n",
    "\n",
    "</div>\n",
    "\n",
    "References:\n",
    "- https://spark.apache.org/docs/latest/api/python/reference/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sW-71-ooNLfI",
    "outputId": "dc332927-9288-4799-d1d6-0173ff56b937"
   },
   "outputs": [],
   "source": [
    "!apt-get install openjdk-17-jdk-headless -qq > /dev/null\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.5.7/spark-3.5.7-bin-hadoop3.tgz\n",
    "!tar xf spark-3.5.7-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.7-bin-hadoop3\"\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(os.environ[\"SPARK_HOME\"], \"bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Vhltz95NKFw"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
    "# \"local\" un solo hilo\n",
    "# \"local[2]\" 2 hilos\n",
    "# \"local[*]\" tantos hilos como cores tiene la máquina\n",
    "#conf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0GsQcjjNOep"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "# Si ya existe un SparkContext, no se crea uno nuevo\n",
    "#sc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzxAEgIDNlC8"
   },
   "outputs": [],
   "source": [
    "# data = sc.textFile(\"sample_data/README.md\")\n",
    "data = sc.textFile(\"sample_data/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data) # Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data) #!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in data:\n",
    "    print(line) #!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9UAF9qCNv3R",
    "outputId": "02635ade-6584-40d2-f3e5-694b55690758"
   },
   "outputs": [],
   "source": [
    "for line in data.collect():\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pL-AqSQ2OBE5"
   },
   "outputs": [],
   "source": [
    "data2 = sc.parallelize([\"pandas\", \"i like pandas\"])\n",
    "type(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UX0vaZnIOYcQ",
    "outputId": "06053c15-523d-49f3-a0ad-33db2a3186b9"
   },
   "outputs": [],
   "source": [
    "len(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WW6z-j7wyE5W"
   },
   "source": [
    "# RDD\n",
    "A **Resilient Distributed Dataset (RDD)**, the basic abstraction in Spark.\n",
    "\n",
    "\n",
    "- https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations\n",
    "- **Actions** are operations that give non-RDD values. The values of action are stored to drivers or to the external storage system. It brings laziness of RDD into motion.<br/>\n",
    "```python\n",
    "lines.count() -> int\n",
    "```\n",
    "\n",
    "- **Transformations** are functions that produces new RDD from the existing RDDs. It takes RDD as input and produces one or more RDD as output. They are operations on RDDs that return a new RDD. As discussed in “Lazy Evaluation” , transformed RDDs are computed lazily, only when you use them in an action. Many transformations are element-wise; that is, they work on one element at a time; but this is not true for all transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlBBQW63Ookh"
   },
   "source": [
    "**Transformations** : Filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJ0o3yFiOc31",
    "outputId": "4fc86c2a-3102-42e9-b3d7-673399d3730a"
   },
   "outputs": [],
   "source": [
    "inputRDD = sc.textFile(\"sample_data/README.md\")\n",
    "samplesRDD = inputRDD.filter(lambda x: \"sample\" in x)\n",
    "type(samplesRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFhQOwv2PPH9"
   },
   "source": [
    "## Actions\n",
    "They are the operations that return a final value to the driver program or write data to an external storage system. Actions force the evaluation of the transformations required for the RDD they were called on, since they need to actually produce output\n",
    "\n",
    "- collect()\n",
    "- count()\n",
    "- countByValue()\n",
    "- take(num)\n",
    "- top(num)\n",
    "- takeOrdered(num)(ordering)\n",
    "- takeSample(...)\n",
    "- reduce()\n",
    "- fold\n",
    "- aggregate\n",
    "- foreach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in inputRDD.collect():\n",
    "  print(line)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86Z8bXY0PJzN",
    "outputId": "8dc2bb74-ed52-4387-a87f-675dd6350aea"
   },
   "outputs": [],
   "source": [
    "print(\"Total Input: %i \"%samplesRDD.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in samplesRDD.take(3):\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesRDD.top(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VMY9vdprychc",
    "outputId": "5fe28a4c-2916-4f67-a001-a39d7af36897"
   },
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1, 2, 2, 2])\n",
    "nums.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuxGndVBwVi-"
   },
   "source": [
    "**reduce()**, which takes a function that operates on two elements of the type in your RDD and returns a new element of the same type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5j0fs4Lu5pT",
    "outputId": "43cd6087-ba6b-4b9d-e91e-98f47a157232"
   },
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4])\n",
    "sum = nums.reduce(lambda x, y: x + y)\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pcQv1YgxFBO"
   },
   "source": [
    "takes a neutral “zero value” to be used for the initial call on each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dal-nK3YwYKw",
    "outputId": "eeb09188-dc3a-463e-a009-72f21a35c1a5"
   },
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4])\n",
    "print(nums.getNumPartitions())\n",
    "nums = sc.parallelize([1, 2, 3, 4],4)\n",
    "print(nums.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicio: **1**\n",
    "```\n",
    "(1 + 1) = 2\n",
    "(2 + 2) = 4\n",
    "(4 + 3) = 7\n",
    "(7 + 4) = 11\n",
    "````\n",
    "\n",
    "Resultado final = 11 + 1 (zeroValue final) = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4],2)\n",
    "sum = nums.fold(1,lambda x, y: x + y)\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1+1+2) + (1+3+4) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5rR0xiSxMf4"
   },
   "source": [
    "aggregate() function frees us from the constraint of having the return be the same type as the RDD we are working on.<br/>\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.aggregate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZX2ZGSwwjdV",
    "outputId": "c1c23489-501d-406f-8320-f803b9bb902a"
   },
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4],2)\n",
    "\n",
    "seqOp = lambda acc, num: (acc[0] + num, acc[1] + 1) # a function used to accumulate results within a partition\n",
    "combOp = lambda partition_1, partition_2: (partition_1[0] + partition_2[0], partition_1[1] + partition_2[1]) #an associative function used to combine results from different partitions\n",
    "\n",
    "sumCount = nums.aggregate((0, 0),seqOp,combOp) #ZeroValue,Inside the partition, Combining partitions\n",
    "\n",
    "print(type(sumCount))\n",
    "print(len(sumCount))\n",
    "print(sumCount)\n",
    "print(\"---\")\n",
    "sc.parallelize([]).aggregate((1, 0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We could have:\n",
    "- partition 1: [1,2] \n",
    "- partition 2: [3,4]\n",
    "\n",
    "SeqOp:\n",
    "- partition 1:\n",
    "```\n",
    "  seqOp((0,0),1) = (0+1,0+1) = (1,1)\n",
    "  seqOp((1,1),2) = (1+2,1+1) = (3,2)\n",
    "```\n",
    "- partition 2: \n",
    "```\n",
    "  seqOp((0,0),3) = (0+3,0+1) = (3,1)\n",
    "  seqOp((3,1),4) = (3+4,1+1) = (7,2)\n",
    "```\n",
    "\n",
    "Combop: \n",
    "````\n",
    "combOp((3,2),(7,2)) = (3+7, 2+2) = (10,4)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflexión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5 ,6, 7, 8, 9, 10], 3) #3 partitions\n",
    "rdd.glom().collect() #Glom. Return an RDD created by coalescing all elements within each partition into a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqOp = lambda acc, num: (acc[0] * num, acc[1] + 1) \n",
    "combOp = lambda partition_1, partition_2: (partition_1[0] + partition_2[0], partition_1[1] + partition_2[1]) #an associative function used to combine results from different partitions\n",
    "unk_op = rdd.aggregate(\n",
    "  (0, -1), seqOp, combOp\n",
    ")\n",
    "# result = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actividad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A. Compute [min,max,len] of:\n",
    "ages = [3,23,42,12,34,50,19,97,1,94,35,65,87]\n",
    "\n",
    "#TIP:\n",
    "import math\n",
    "print(math.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B. Compute [sum(sales > 300$),len(sales >300)]\n",
    "sales = [300,200,1094,390,29,320,90,10029]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "Narrow Transformation:\n",
    "- map\n",
    "- flatMap\n",
    "- MapPartition\n",
    "- Filter\n",
    "- Sample\n",
    "- Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4])\n",
    "squared = nums.map(lambda x: x * x)\n",
    "print(squared.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.parallelize([\"hello world\", \"hi Pepe, how are you?\"])\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "print(words.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a new RDD by applying a function to each partition of this RDD.\n",
    "rdd = sc.parallelize([(1, 2), (3, 4), (5, 6), (7, 8)], 2)\n",
    "print(rdd.glom().collect() )\n",
    "def average_partition(iterator):\n",
    "     x_sum = 0\n",
    "     y_sum = 0\n",
    "     count = 0\n",
    "     for (x, y) in iterator:\n",
    "         x_sum += x\n",
    "         y_sum += y\n",
    "         count += 1\n",
    "     yield (x_sum/count, y_sum/count)\n",
    "\n",
    "avg_rdd = rdd.mapPartitions(average_partition)\n",
    "print(avg_rdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.parallelize([\"hello world\", \"hi Pepe, who are you?\"])\n",
    "lines.filter(lambda x: \"wo\" in x).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sample.html\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "rdd.sample(False,0.5,seed=1).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Union(), disctint(), intersection(), subtract(), cartesian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.parallelize([\"hello world\", \"hi Pepe, who are you?\"])\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "uRDD = rdd.union(lines)\n",
    "uRDD.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actividad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C. Count the number of words in the file after transforming to lowercase and removing words with less than 4 characters; and count the number of discting words.\n",
    "#TIP: map, flatmap, filter, \n",
    "# distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JD8k1yzz83O"
   },
   "source": [
    "# **Pair RDDs**\n",
    "\n",
    "Pair RDDs are a useful building block in many programs, as they expose operations that allow you to act on each key in parallel or regroup data across the network. For example, pair RDDs have a reduceByKey() method that can aggregate data separately for each key, and a join() method that can merge two RDDs together by grouping elements with the same key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRWTG_611a4w"
   },
   "source": [
    "\n",
    "Wide transformation:\n",
    "\n",
    "- keys()\n",
    "- values()\n",
    "- reduceByKey(func)\n",
    "- groupByKey()\n",
    "- combineByKey(...)\n",
    "- mapValues(func)\n",
    "- flatMapValues(func)\n",
    "- sortByKey()\n",
    "- countByKey()\n",
    "- collectAsMap()\n",
    "- lookup(key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Syjw5F26xeq8",
    "outputId": "e2f92ce9-6624-4d49-c518-55c207c5796b"
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"sample_data/README.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = lines.map(lambda x: (x.split(\" \")[0], x))\n",
    "pairs.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.keys().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9CkTMuHW0SD-",
    "outputId": "95e66fea-01d6-4f53-f7f3-8c5f9ae34f6e"
   },
   "outputs": [],
   "source": [
    "pairs.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hgiCX0Gn1oIx",
    "outputId": "038c46c8-a6de-4389-bb13-136eba5e7f4a"
   },
   "outputs": [],
   "source": [
    "pairs.keys().distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQfEstOt1uM0"
   },
   "outputs": [],
   "source": [
    "## Reflexion\n",
    "words = lines.flatMap(lambda x: x.split(\" \"))\n",
    "result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ktlHzctL2dRf",
    "outputId": "5cfd81c8-208d-41ef-d616-8604ea9b26c2"
   },
   "outputs": [],
   "source": [
    "for kv in result.sortByKey().collect():\n",
    "  print(kv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jwL5UMs56yY"
   },
   "source": [
    "**Join** operator is an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxHaKmpw5Eef"
   },
   "outputs": [],
   "source": [
    "data1 = [(\"a\", 3), (\"b\", 4), (\"a\", 1)]\n",
    "data2 = [(\"a\", 5), (\"b\", 1), (\"c\", 1)]\n",
    "d1 = sc.parallelize(data1)\n",
    "d2 = sc.parallelize(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-gzNzqxX5VNZ",
    "outputId": "3e51177b-7c61-4e5b-9516-3d0229ba8e94"
   },
   "outputs": [],
   "source": [
    "for kv in d1.join(d2).collect():\n",
    "  print(kv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ybjgc54W6PtA"
   },
   "source": [
    "leftOuterJoin(other) ,  rightOuterJoin(other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hUX574A16Rh5",
    "outputId": "58d63fb8-bead-4587-f455-640b0092396a"
   },
   "outputs": [],
   "source": [
    "for kv in d1.leftOuterJoin(d2).collect():\n",
    "  print(kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s8XooVdV6ZUe",
    "outputId": "a0c012cc-9841-4088-eadc-7e3e8eec539a"
   },
   "outputs": [],
   "source": [
    "for kv in d1.rightOuterJoin(d2).collect():\n",
    "  print(kv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lookup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = range(100)\n",
    "l1 = range(1,101)\n",
    "rdd = sc.parallelize(zip(l, l1), 10)\n",
    "print(rdd.take(5))\n",
    "rdd.lookup(42)  # slow\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted = rdd.sortByKey()\n",
    "sorted.lookup(42)  # fast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"PYTHONHASHSEED\"]=\"1\" #Spark sessions: coherencia entre tipos \n",
    "\n",
    "rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n",
    "list(rdd2.lookup(('a', 'b'))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxrq4UJ70Jtz"
   },
   "source": [
    "# Example: Page Rank Algorithm Implementation\n",
    "\n",
    "$$PageRank(A) = \\frac{(1 - d)}{N} + d * \\sum_{B\\in in(A)} \\frac{PageRank(B)}{L(B)}$$\n",
    "\n",
    "\n",
    "Donde:\n",
    "\n",
    "- A y B son páginas\n",
    "- `PageRank(A)` es el valor de PageRank para la página A.\n",
    "- `d` es el factor de amortiguación (generalmente se establece en 0.85 en la práctica).\n",
    "- `N` es el número total de páginas en la red.\n",
    "- `Σ` representa la suma sobre todas las páginas B que enlazan a la página A.\n",
    "- in(A) es el conjunto de páginas que enlazan a la página A.\n",
    "- `PageRank(B)` es el valor de PageRank de la página B.\n",
    "- `L(B)` es el número de enlaces salientes desde la página B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDRJQZo82Kgo"
   },
   "source": [
    "Supongamos que tenemos cuatro páginas web (A, B, C y D) en una red y que inicialmente todas tienen un PageRank igual. El factor de amortiguación (d) es 0.85.\n",
    "\n",
    "Relaciones:\n",
    "\n",
    "- A <- B\n",
    "- B <- A, C\n",
    "- C <- B\n",
    "- D <- B\n",
    "\n",
    "Iteraciones:\n",
    "\n",
    "* Iteración 0 (valores iniciales):\n",
    "\n",
    "\n",
    "\n",
    "PageRank(A) = PageRank(B) = PageRank(C) = PageRank(D) = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_keMwXo2OF4"
   },
   "source": [
    "* Iteración 1:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "PageRank(A) & = \\frac{(1 - 0.85)}{4} + 0.85 \\cdot \\frac{PageRank(B)}{1} \\\\\n",
    "& = 0.0375 + 0.85 \\cdot 0.25 = 0.2875\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "PageRank(B) & = \\frac{(1 - 0.85)}{4} + 0.85 \\cdot \\left(\\frac{PageRank(A)}{1} + \\frac{PageRank(C)}{1}\\right) \\\\\n",
    "& = 0.0375 + 0.85 \\cdot (0.2875 + 0.25) = 0.675\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "PageRank(C) & = \\frac{(1 - 0.85)}{4} + 0.85 \\cdot \\frac{PageRank(B)}{1} \\\\\n",
    "& = 0.0375 + 0.85 \\cdot 0.675 = 0.6025\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "PageRank(D) & = \\frac{(1 - 0.85)}{4} + 0.85 \\cdot \\frac{PageRank(B)}{1} \\\\\n",
    "& = 0.0375 + 0.85 \\cdot 0.675 = 0.6025\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REF: https://github.com/Proxy08/PageRank/blob/main/pagerank-spark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sow0VuvJ0JK1",
    "outputId": "512c1314-82d2-4411-c652-31f5baea93ad"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PageRank\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "lines = spark.read.text(\"pageRank_data.txt\")\n",
    "lines = lines.rdd.map(lambda r: r[0])\n",
    "for i in lines.collect():\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines.rdd.map(lambda r: r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lines.collect():\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loads all URLs from input file and initialize their neighbors.\n",
    "links = lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in links.collect():\n",
    "  print(i[0])\n",
    "  for j in i[1]:\n",
    "    print(\"\\t\",j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loads all URLs with other URL(s) link to from input file and initialize ranks of them to one.\n",
    "ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ranks.collect():\n",
    "  print(i[0],i[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = links.join(ranks)\n",
    "t.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribs = links.join(ranks).flatMap(lambda url_urls_rank: computeContribs(\n",
    "    url_urls_rank[1][0], url_urls_rank[1][1]  # type: ignore[arg-type]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = links.join(ranks).flatMap(lambda url_urls_rank: (url_urls_rank[1][0], url_urls_rank[1][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in a.collect():\n",
    " if (type(i)!=float):\n",
    "  for x in i:\n",
    "    print(x)\n",
    " else:\n",
    "  print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribs.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in contribs.collect():\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "for i in ranks.collect():\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "for iteration in range(5):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    contribs = links.join(ranks).flatMap(lambda url_urls_rank: computeContribs(\n",
    "        url_urls_rank[1][0], url_urls_rank[1][1]  # type: ignore[arg-type]\n",
    "    ))\n",
    "\n",
    "    # Re-calculates URL ranks based on neighbor contributions.\n",
    "    ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "\n",
    "# Collects all URL ranks and dump them to console.\n",
    "for (link, rank) in ranks.collect():\n",
    "    print(\"%s has rank: %s.\" % (link, rank))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D. Keys and values:\n",
    "products =[(\"manzana\", 1), (\"banana\", 2), (\"manzana\", 3), (\"pera\", 2),(\"pera\", 8)]\n",
    "# discover keys, values and counts them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E. Sum the prices of each product:\n",
    "products =[(\"manzana\", 1), (\"banana\", 2), (\"manzana\", 3), (\"pera\", 2),(\"pera\", 8)]\n",
    "# TIP: reducebykey, groupbykey,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F. Compute the average of each group:\n",
    "groups = [(\"A\", 10), (\"A\", 20), (\"B\", 5), (\"B\", 15)]\n",
    "#TIP: mappartition, reducebykey, map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faker faker-commerce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos un archivo de transacciones\n",
    "```csv\n",
    "order_id, product_id, quantity, price, status\n",
    "1, 101, 2, 15.99, COMPLETE\n",
    "2, 103, 1, 29.99, PENDING\n",
    "3, 101, 1, 15.99, CANCELED\n",
    "...\n",
    "```\n",
    "\n",
    "llamado `orders.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "size = 1000\n",
    "states = [\"COMPLETE\",\"PENDING\",\"CANCELED\"]\n",
    "product_id = np.random.choice(np.random.randint(200,10090,size=500),size)\n",
    "quantity = np.random.randint(1,10,size)\n",
    "price = np.random.randint(1,3000,size)\n",
    "status = np.random.choice(states,size,p=[0.7,0.25,0.05])\n",
    "pd.DataFrame({\"order_id\":range(size),\"product_id\":product_id, \"quantity\":quantity, \"price\":price, \"status\":status}).to_csv(\"orders.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y un archivo de productos, llamado `products.csv`:\n",
    "\n",
    "```csv\n",
    "product_id, product_name, category, company\n",
    "101, Widget A, Electronics, Leach-Smith\n",
    "103, Widget B, Home, Hopkins Inc\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import faker_commerce\n",
    "Faker.seed(1)\n",
    "\n",
    "u_product= np.unique(product_id)\n",
    "size = len(u_product)\n",
    "print(size)\n",
    "\n",
    "# category= [\"Automative\",\"Home\",\"Electronics\",\"Retail\",\"Health Care\",\"Books\",\"Media\"]\n",
    "fake = Faker()\n",
    "fake.add_provider(faker_commerce.Provider)\n",
    "    \n",
    "pd.DataFrame({\"product_id\":u_product, \n",
    "              \"product_name\":[fake.name() for _ in range(size)], \n",
    "              \"category\":[fake.ecommerce_category() for _ in range(size)], \n",
    "              \"company\":[fake.company() for _ in range(size)], \n",
    "              }).to_csv(\"products.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tareas\n",
    "- A. Combinar ambos ficheros \n",
    "- B. Controlar la facturación:\n",
    "  - a. Contar la cantidad de pedidos en cada estado (COMPLETE, CANCELED, PENDING).\n",
    "  - b. Total de Ingresos por Producto (Filtrar solo los pedidos en estado COMPLETE) y calcular el ingreso total para cada product_id multiplicando quantity por price. Mapea los resultados a una estructura con el nombre del producto y la categoría.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my3110",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
