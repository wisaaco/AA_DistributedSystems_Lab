{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencies:\n",
    "- https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sW-71-ooNLfI",
    "outputId": "dc332927-9288-4799-d1d6-0173ff56b937"
   },
   "outputs": [],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6Vhltz95NKFw"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J0GsQcjjNOep"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/05 11:10:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kzxAEgIDNlC8"
   },
   "outputs": [],
   "source": [
    "# lines = sc.textFile(\"sample_data/README.md\")\n",
    "lines = sc.textFile(\"steps-hadoop.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qgsZgtPCNqPK"
   },
   "outputs": [],
   "source": [
    "# samplelines = lines.filter(lambda line: \"sample\" in line)\n",
    "samplelines = lines.filter(lambda line: \"in\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cL-8WvUBNuUZ",
    "outputId": "40859857-4774-43cf-fafe-51f408559a23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samplelines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9UAF9qCNv3R",
    "outputId": "02635ade-6584-40d2-f3e5-694b55690758"
   },
   "outputs": [],
   "source": [
    "for line in samplelines.collect():\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLT0AhMvN6J9",
    "outputId": "c4ae649f-0a5d-4571-d5af-0c184b707446"
   },
   "outputs": [],
   "source": [
    "for line in samplelines.toLocalIterator():\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pL-AqSQ2OBE5"
   },
   "outputs": [],
   "source": [
    "lines = sc.parallelize([\"pandas\", \"i like pandas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UX0vaZnIOYcQ",
    "outputId": "06053c15-523d-49f3-a0ad-33db2a3186b9"
   },
   "outputs": [],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WW6z-j7wyE5W"
   },
   "source": [
    "A **Resilient Distributed Dataset (RDD)**, the basic abstraction in Spark.\n",
    "\n",
    "https://spark.apache.org/docs/1.5.1/api/python/pyspark.html#pyspark.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9SnrbNdOZOE",
    "outputId": "759c972a-bff9-4485-9705-040236eef69d"
   },
   "outputs": [],
   "source": [
    "lines.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "eHs4F3bjObPm",
    "outputId": "06129e7b-5abf-4b0d-8003-60830426033d"
   },
   "outputs": [],
   "source": [
    "#len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlBBQW63Ookh"
   },
   "source": [
    "**Transformations** are operations on RDDs that return a new RDD. As discussed in “Lazy Evaluation” , transformed RDDs are computed lazily, only when you use them in an action. Many transformations are element-wise; that is, they work on one element at a time; but this is not true for all transformations.\n",
    "\n",
    "Filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJ0o3yFiOc31",
    "outputId": "4fc86c2a-3102-42e9-b3d7-673399d3730a"
   },
   "outputs": [],
   "source": [
    "inputRDD = sc.textFile(\"sample_data/README.md\")\n",
    "samplesRDD = inputRDD.filter(lambda x: \"sample\" in x)\n",
    "type(samplesRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXkdTM2NO7NV"
   },
   "source": [
    "Union(), disctint(), intersection(), subtract(), cartesian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mU6B9ZfCOyJz",
    "outputId": "6f70ce7f-236a-4479-c433-b4a15dac1426"
   },
   "outputs": [],
   "source": [
    "esRDD = inputRDD.filter(lambda x: \"es\" in x)\n",
    "esRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ELnjlfCLPAmC",
    "outputId": "4867d0b0-ae67-4271-e719-7fed4901c454"
   },
   "outputs": [],
   "source": [
    "uRDD = esRDD.union(samplesRDD)\n",
    "uRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZvsbj7PuhZ_"
   },
   "source": [
    "Element-wise **transformations**\n",
    "The two most common transformations you will likely be using are map() and filter(), distinct(), sample(withReplacement=Boolean, fraction=Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0aFX0v38PfFc",
    "outputId": "8bd23622-e53e-4e04-90e4-bdee5f40a10c"
   },
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4])\n",
    "squared = nums.map(lambda x: x * x).collect()\n",
    "for num in squared:\n",
    "  print(\"%i\"%num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAf9bahgu2sY"
   },
   "source": [
    "Sometimes we want to produce multiple output elements for each input element. The operation to do this is called flatMap()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "o7vmPh3Euj6j",
    "outputId": "054812c2-2f27-42ba-aeb5-a6a83d9cc1a4"
   },
   "outputs": [],
   "source": [
    "lines = sc.parallelize([\"hello world\", \"hi\"])\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "words.first() # returns \"hello\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFhQOwv2PPH9"
   },
   "source": [
    "## **Actions**\n",
    "They are the operations that return a final value to the driver program or write data to an external storage system. Actions force the evaluation of the transformations required for the RDD they were called on, since they need to actually produce output\n",
    "\n",
    "- collect()\n",
    "- count()\n",
    "- countByValue()\n",
    "- take(num)\n",
    "- top(num)\n",
    "- takeOrdered(num)(ordering)\n",
    "- takeSample(...)\n",
    "- reduce()\n",
    "- fold\n",
    "- aggregate\n",
    "- foreach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86Z8bXY0PJzN",
    "outputId": "8dc2bb74-ed52-4387-a87f-675dd6350aea"
   },
   "outputs": [],
   "source": [
    "print(\"Total Input: %i \"%uRDD.count())\n",
    "for line in uRDD.take(3):\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VMY9vdprychc",
    "outputId": "5fe28a4c-2916-4f67-a001-a39d7af36897"
   },
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1, 2, 2, 2])\n",
    "nums.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuxGndVBwVi-"
   },
   "source": [
    "**reduce()**, which takes a function that operates on two elements of the type in your RDD and returns a new element of the same type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5j0fs4Lu5pT",
    "outputId": "43cd6087-ba6b-4b9d-e91e-98f47a157232"
   },
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4])\n",
    "sum = nums.reduce(lambda x, y: x + y)\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pcQv1YgxFBO"
   },
   "source": [
    "takes a “zero value” to be used for the initial call on each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dal-nK3YwYKw",
    "outputId": "eeb09188-dc3a-463e-a009-72f21a35c1a5"
   },
   "outputs": [],
   "source": [
    "sum = nums.fold(1,lambda x, y: x + y)\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5rR0xiSxMf4"
   },
   "source": [
    "aggregate() function frees us from the constraint of having the return be the same type as the RDD we are working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZX2ZGSwwjdV",
    "outputId": "c1c23489-501d-406f-8320-f803b9bb902a"
   },
   "outputs": [],
   "source": [
    "seqOp = lambda acc, value: (acc[0] + value, acc[1] + 1)\n",
    "combOp = lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
    "sumCount = nums.aggregate((0, 0),seqOp,combOp)\n",
    "\n",
    "print(sumCount[0])\n",
    "print(sumCount[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JD8k1yzz83O"
   },
   "source": [
    "# **Pair RDDs**\n",
    "\n",
    "Pair RDDs are a useful building block in many programs, as they expose operations that allow you to act on each key in parallel or regroup data across the network. For example, pair RDDs have a reduceByKey() method that can aggregate data separately for each key, and a join() method that can merge two RDDs together by grouping elements with the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Syjw5F26xeq8",
    "outputId": "e2f92ce9-6624-4d49-c518-55c207c5796b"
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"sample_data/README.md\")\n",
    "pairs = lines.map(lambda x: (x.split(\" \")[0], x))\n",
    "pairs.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRWTG_611a4w"
   },
   "source": [
    "- reduceByKey(func)\n",
    "- groupByKey()\n",
    "- combineByKey(...)\n",
    "- mapValues(func)\n",
    "- flatMapValues(func)\n",
    "- keys()\n",
    "- values()\n",
    "- sortByKey()\n",
    "- countByKey()\n",
    "- collectAsMap()\n",
    "- lookup(key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9CkTMuHW0SD-",
    "outputId": "95e66fea-01d6-4f53-f7f3-8c5f9ae34f6e"
   },
   "outputs": [],
   "source": [
    "pairs.keys().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hgiCX0Gn1oIx",
    "outputId": "038c46c8-a6de-4389-bb13-136eba5e7f4a"
   },
   "outputs": [],
   "source": [
    "pairs.keys().distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQfEstOt1uM0"
   },
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda x: x.split(\" \"))\n",
    "result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ktlHzctL2dRf",
    "outputId": "5cfd81c8-208d-41ef-d616-8604ea9b26c2"
   },
   "outputs": [],
   "source": [
    "for kv in result.sortByKey().collect():\n",
    "  print(kv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jwL5UMs56yY"
   },
   "source": [
    "The simple **join** operator is an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxHaKmpw5Eef"
   },
   "outputs": [],
   "source": [
    "data1 = [(\"a\", 3), (\"b\", 4), (\"a\", 1)]\n",
    "data2 = [(\"a\", 5), (\"b\", 1), (\"c\", 1)]\n",
    "d1 = sc.parallelize(data1)\n",
    "d2 = sc.parallelize(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-gzNzqxX5VNZ",
    "outputId": "3e51177b-7c61-4e5b-9516-3d0229ba8e94"
   },
   "outputs": [],
   "source": [
    "for kv in d1.join(d2).collect():\n",
    "  print(kv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ybjgc54W6PtA"
   },
   "source": [
    "leftOuterJoin(other) ,  rightOuterJoin(other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hUX574A16Rh5",
    "outputId": "58d63fb8-bead-4587-f455-640b0092396a"
   },
   "outputs": [],
   "source": [
    "for kv in d1.leftOuterJoin(d2).collect():\n",
    "  print(kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s8XooVdV6ZUe",
    "outputId": "a0c012cc-9841-4088-eadc-7e3e8eec539a"
   },
   "outputs": [],
   "source": [
    "for kv in d1.rightOuterJoin(d2).collect():\n",
    "  print(kv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxrq4UJ70Jtz"
   },
   "source": [
    "# Activity\n",
    "\n",
    "$$PageRank(A) = \\frac{(1 - d)}{N} + d * \\sum_{B\\in in(A)} \\frac{PageRank(B)}{L(B)}$$\n",
    "\n",
    "\n",
    "Donde:\n",
    "\n",
    "- A y B son páginas\n",
    "- `PageRank(A)` es el valor de PageRank para la página A.\n",
    "- `d` es el factor de amortiguación (generalmente se establece en 0.85 en la práctica).\n",
    "- `N` es el número total de páginas en la red.\n",
    "- `Σ` representa la suma sobre todas las páginas B que enlazan a la página A.\n",
    "- in(A) es el conjunto de páginas que enlazan a la página A.\n",
    "- `PageRank(B)` es el valor de PageRank de la página B.\n",
    "- `L(B)` es el número de enlaces salientes desde la página B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDRJQZo82Kgo"
   },
   "source": [
    "Supongamos que tenemos cuatro páginas web (A, B, C y D) en una red y que inicialmente todas tienen un PageRank igual. El factor de amortiguación (d) es 0.85.\n",
    "\n",
    "Relaciones:\n",
    "\n",
    "- A <- B\n",
    "- B <- A, C\n",
    "- C <- B\n",
    "- D <- B\n",
    "\n",
    "Iteraciones:\n",
    "\n",
    "* Iteración 0 (valores iniciales):\n",
    "\n",
    "\n",
    "\n",
    "PageRank(A) = PageRank(B) = PageRank(C) = PageRank(D) = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_keMwXo2OF4"
   },
   "source": [
    "* Iteración 1:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "PageRank(A) & = \\frac{(1 - 0.85)}{4} + 0.85 \\cdot \\frac{PageRank(B)}{1} \\\\\n",
    "& = 0.0375 + 0.85 \\cdot 0.25 = 0.2875\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "PageRank(B) & = \\frac{(1 - 0.85)}{4} + 0.85 \\cdot \\left(\\frac{PageRank(A)}{1} + \\frac{PageRank(C)}{1}\\right) \\\\\n",
    "& = 0.0375 + 0.85 \\cdot (0.2875 + 0.25) = 0.675\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "PageRank(C) & = \\frac{(1 - 0.85)}{4} + 0.85 \\cdot \\frac{PageRank(B)}{1} \\\\\n",
    "& = 0.0375 + 0.85 \\cdot 0.675 = 0.6025\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "PageRank(D) & = \\frac{(1 - 0.85)}{4} + 0.85 \\cdot \\frac{PageRank(B)}{1} \\\\\n",
    "& = 0.0375 + 0.85 \\cdot 0.675 = 0.6025\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REF: https://github.com/Proxy08/PageRank/blob/main/pagerank-spark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sow0VuvJ0JK1",
    "outputId": "512c1314-82d2-4411-c652-31f5baea93ad"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PageRank\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "lines = spark.read.text(\"pageRank_data.txt\")\n",
    "lines = lines.rdd.map(lambda r: r[0])\n",
    "for i in lines.collect():\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines.rdd.map(lambda r: r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lines.collect():\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loads all URLs from input file and initialize their neighbors.\n",
    "links = lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in links.collect():\n",
    "  print(i[0])\n",
    "  for j in i[1]:\n",
    "    print(\"\\t\",j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loads all URLs with other URL(s) link to from input file and initialize ranks of them to one.\n",
    "ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ranks.collect():\n",
    "  print(i[0],i[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = links.join(ranks)\n",
    "t.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribs = links.join(ranks).flatMap(lambda url_urls_rank: computeContribs(\n",
    "    url_urls_rank[1][0], url_urls_rank[1][1]  # type: ignore[arg-type]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = links.join(ranks).flatMap(lambda url_urls_rank: (url_urls_rank[1][0], url_urls_rank[1][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in a.collect():\n",
    " if (type(i)!=float):\n",
    "  for x in i:\n",
    "    print(x)\n",
    " else:\n",
    "  print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribs.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in contribs.collect():\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "for i in ranks.collect():\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "for iteration in range(5):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    contribs = links.join(ranks).flatMap(lambda url_urls_rank: computeContribs(\n",
    "        url_urls_rank[1][0], url_urls_rank[1][1]  # type: ignore[arg-type]\n",
    "    ))\n",
    "\n",
    "    # Re-calculates URL ranks based on neighbor contributions.\n",
    "    ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "\n",
    "# Collects all URL ranks and dump them to console.\n",
    "for (link, rank) in ranks.collect():\n",
    "    print(\"%s has rank: %s.\" % (link, rank))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my3110",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
